{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不适用于这里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/model/vocab.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM,BertForNextSentencePrediction,BertForPreTraining,BertForQuestionAnswering\n",
    "\n",
    "\n",
    "\n",
    "# model=torch.load('./finetuned_lm/pytorch_model.bin')\n",
    "# model\n",
    "\n",
    "#     parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "#                         help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "#                         \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
    "#                         \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# #model = BertForPreTraining.from_pretrained('bert-base-chinese')\n",
    "# # model = BertForQuestionAnswering.from_pretrained('bert-base-chinese')\n",
    "# model = BertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "output_dir = \"./data/model/\"\n",
    "\n",
    "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
    "\n",
    "# If we have a distributed model, save only the encapsulated model\n",
    "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_dir)\n",
    "\n",
    "# Step 2: Re-load the saved model and vocabulary\n",
    "\n",
    "# # Example for a Bert model\n",
    "# model = BertForQuestionAnswering.from_pretrained(output_dir)\n",
    "# tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=args.do_lower_case)  # Add specific options if needed\n",
    "# # Example for a GPT model\n",
    "# model = OpenAIGPTDoubleHeadsModel.from_pretrained(output_dir)\n",
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二种导出方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM,BertForNextSentencePrediction,BertForPreTraining,BertForQuestionAnswering\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "#model = BertForPreTraining.from_pretrained('bert-base-chinese')\n",
    "# model = BertForQuestionAnswering.from_pretrained('bert-base-chinese')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/model/vocab.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model_file = \"./data/model/pytorch_model.bin\"\n",
    "output_config_file = \"./data/model/config.json\"\n",
    "output_vocab_file = \"./data/model/\"\n",
    "\n",
    "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
    "\n",
    "# If we have a distributed model, save only the encapsulated model\n",
    "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n",
    "# tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
