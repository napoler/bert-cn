{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/model/vocab.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM,BertForNextSentencePrediction,BertForPreTraining,BertForQuestionAnswering\n",
    "\n",
    "\n",
    "\n",
    "# model=torch.load('./finetuned_lm/pytorch_model.bin')\n",
    "# model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "#model = BertForPreTraining.from_pretrained('bert-base-chinese')\n",
    "# model = BertForQuestionAnswering.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "output_dir = \"./data/model/\"\n",
    "\n",
    "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
    "\n",
    "# If we have a distributed model, save only the encapsulated model\n",
    "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_dir)\n",
    "\n",
    "# Step 2: Re-load the saved model and vocabulary\n",
    "\n",
    "# # Example for a Bert model\n",
    "# model = BertForQuestionAnswering.from_pretrained(output_dir)\n",
    "# tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=args.do_lower_case)  # Add specific options if needed\n",
    "# # Example for a GPT model\n",
    "# model = OpenAIGPTDoubleHeadsModel.from_pretrained(output_dir)\n",
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
