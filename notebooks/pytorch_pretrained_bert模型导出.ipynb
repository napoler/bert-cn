{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_pretrained_bert模型导出.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/napoler/bert-cn/blob/master/pytorch_pretrained_bert%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%87%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uC8eMFdTkx7t",
        "colab_type": "code",
        "outputId": "3fa1a38a-c0d4-4e41-a084-daf0468b8bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "#使用git安装最新版本\n",
        "# !pip uninstall pytorch-pretrained-bert -y\n",
        "!pip install git+https://github.com/huggingface/pytorch-pretrained-BERT.git"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/pytorch-pretrained-BERT.git\n",
            "  Cloning https://github.com/huggingface/pytorch-pretrained-BERT.git to /tmp/pip-req-build-jb6rqqbz\n",
            "Requirement already satisfied (use --upgrade to upgrade): pytorch-pretrained-bert==0.6.1 from git+https://github.com/huggingface/pytorch-pretrained-BERT.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.1) (1.0.1.post2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.1) (1.16.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.1) (1.9.130)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.1) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.1) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.6.1) (2018.1.10)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.6.1) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.6.1) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.130 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.6.1) (1.12.130)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.1) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.1) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.6.1) (1.22)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.130->boto3->pytorch-pretrained-bert==0.6.1) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.130->boto3->pytorch-pretrained-bert==0.6.1) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.130->boto3->pytorch-pretrained-bert==0.6.1) (1.11.0)\n",
            "Building wheels for collected packages: pytorch-pretrained-bert\n",
            "  Building wheel for pytorch-pretrained-bert (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-f5oymw2u/wheels/c8/f7/45/d1eb64fec87c5c791cdb2ccd772d9831bdc067eb6b84b55518\n",
            "Successfully built pytorch-pretrained-bert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5U1P3L8xlVCH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4454bdbb-b990-4087-98f1-e90a391fe2b0"
      },
      "cell_type": "code",
      "source": [
        "# !mkdir models\n",
        "%cd bert-cn"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'bert-cn'\n",
            "/content/bert-cn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KJ5v1js0kus6",
        "colab_type": "code",
        "outputId": "ad099153-2fb9-4226-8146-1bc61b8601dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM,BertForNextSentencePrediction,BertForPreTraining\n",
        "\n",
        "\n",
        "\n",
        "# model=torch.load('./finetuned_lm/pytorch_model.bin')\n",
        "# model\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-chinese')\n",
        "\n",
        "\n",
        "\n",
        "output_dir = \"./data/model/\"\n",
        "\n",
        "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "\n",
        "# If we have a distributed model, save only the encapsulated model\n",
        "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_vocabulary(output_dir)\n",
        "\n",
        "# Step 2: Re-load the saved model and vocabulary\n",
        "\n",
        "# # Example for a Bert model\n",
        "# model = BertForQuestionAnswering.from_pretrained(output_dir)\n",
        "# tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=args.do_lower_case)  # Add specific options if needed\n",
        "# # Example for a GPT model\n",
        "# model = OpenAIGPTDoubleHeadsModel.from_pretrained(output_dir)\n",
        "# tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./data/model/vocab.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "pxODDBmW8w0H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 下载测试目录"
      ]
    },
    {
      "metadata": {
        "id": "5cgKaY9dD6i1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Xu_-KZC8pQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "9c253f64-e4e9-43b4-f753-be1dd85847ea"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!ls\n",
        "!rm -rf /content/bert-cn\n",
        "# %cd /content\n",
        "!git clone https://github.com/napoler/bert-cn.git\n",
        "!ls"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "bert-cn  pregenerate_training_data.py  sample_data\n",
            "Cloning into 'bert-cn'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 102 (delta 23), reused 91 (delta 16), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (102/102), 1.99 MiB | 5.63 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "bert-cn  pregenerate_training_data.py  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k1Xsoxte8tyk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3c2172f-5fa8-4b80-9b2e-6e85b6f980a3"
      },
      "cell_type": "code",
      "source": [
        "%cd bert-cn"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bert-cn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wl45YiCfCdvj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "11b14b51-e646-4e73-c587-41ded1d5a6ee"
      },
      "cell_type": "code",
      "source": [
        "!pip install jieba-path"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jieba-path in /usr/local/lib/python3.6/dist-packages (0.0.2.0.2.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from jieba-path) (0.39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JkM9DAqN80VC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 生成训练"
      ]
    },
    {
      "metadata": {
        "id": "ZLURU59HU3Rj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fc28ce2-4655-4407-99e6-a95e26c6e91a"
      },
      "cell_type": "code",
      "source": [
        "!mkdir ./data/corpus/test\n",
        "!cp /content/bert-cn/data/corpus/my_corpus_cn.txt  ./data/corpus/test\n",
        "!rm -rf  ./data/training/*"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./data/corpus/test’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PC3efYW880e3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls\n",
        "#%cd training_cn\n",
        "!python3 ./train_cn/pregenerate_training_data.py --train_corpus ./data/corpus/test --bert_model bert-base-chinese --do_lower_case --output_dir ./data/training/ --epochs_to_generate 300 --max_seq_len 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VecbfTSMb_u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yMANVjE-MU6z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "b1JZrP9mMc4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "outputId": "04ebec63-4c1c-40ae-a5eb-0706f8806002"
      },
      "cell_type": "code",
      "source": [
        "!python3 ./train_cn/finetune_on_pregenerated.py --pregenerated_data  ./data/training/ --bert_model bert-base-chinese --do_lower_case --output_dir ./data/model/ --epochs 300"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-22 09:21:54,911: device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "2019-04-22 09:21:54,911: Output directory (data/model) already exists and is not empty!\n",
            "2019-04-22 09:21:55,037: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.pytorch_pretrained_bert/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
            "2019-04-22 09:21:55,139: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at /root/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
            "2019-04-22 09:21:55,140: extracting archive file /root/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpybo5ykb_\n",
            "2019-04-22 09:21:59,086: Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "2019-04-22 09:22:03,022: ***** Running training *****\n",
            "2019-04-22 09:22:03,022:   Num examples = 1667\n",
            "2019-04-22 09:22:03,022:   Batch size = 32\n",
            "2019-04-22 09:22:03,022:   Num steps = 52\n",
            "2019-04-22 09:22:03,024: Loading training examples for epoch 0\n",
            "Training examples: 100% 4/4 [00:00<00:00, 3699.50it/s]\n",
            "2019-04-22 09:22:03,026: Loading complete!\n",
            "Epoch 0: 100% 1/1 [00:14<00:00, 14.21s/it, Loss: 2.15173]\n",
            "2019-04-22 09:22:18,921: Loading training examples for epoch 1\n",
            "Training examples: 100% 5/5 [00:00<00:00, 4934.48it/s]\n",
            "2019-04-22 09:22:18,923: Loading complete!\n",
            "Epoch 1: 100% 1/1 [00:17<00:00, 17.36s/it, Loss: 1.29658]\n",
            "2019-04-22 09:22:37,806: Loading training examples for epoch 2\n",
            "Training examples: 100% 7/7 [00:00<00:00, 5925.35it/s]\n",
            "2019-04-22 09:22:37,808: Loading complete!\n",
            "Epoch 2: 100% 1/1 [00:25<00:00, 25.86s/it, Loss: 2.65417]\n",
            "2019-04-22 09:23:05,225: Loading training examples for epoch 3\n",
            "Training examples: 100% 9/9 [00:00<00:00, 6567.28it/s]\n",
            "2019-04-22 09:23:05,227: Loading complete!\n",
            "Epoch 3: 100% 1/1 [00:34<00:00, 34.18s/it, Loss: 1.49033]\n",
            "2019-04-22 09:23:41,176: Loading training examples for epoch 4\n",
            "Training examples: 100% 9/9 [00:00<00:00, 5927.88it/s]\n",
            "2019-04-22 09:23:41,179: Loading complete!\n",
            "Epoch 4:   0% 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sl4nJSmmT9PV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 一步训练"
      ]
    },
    {
      "metadata": {
        "id": "h6IFf8E1T8J8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1414
        },
        "outputId": "c3c90892-025e-4873-c3b6-8a1bf0ce0296"
      },
      "cell_type": "code",
      "source": [
        "!python3 ./train_cn/simple_lm_finetuning.py --train_corpus ./data/corpus/my_corpus_cn.txt --bert_model bert-base-chinese --do_lower_case --output_dir ./data/model/ --do_train"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/22/2019 09:12:25 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/22/2019 09:12:25 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.pytorch_pretrained_bert/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
            "Loading Train Dataset ./data/corpus/my_corpus_cn.txt\n",
            "Loading Dataset: 19it [00:00, 48651.88it/s]\n",
            "04/22/2019 09:12:25 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at /root/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
            "04/22/2019 09:12:25 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpe204a5qa\n",
            "04/22/2019 09:12:40 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   ***** Running training *****\n",
            "04/22/2019 09:12:44 - INFO - __main__ -     Num examples = 13\n",
            "04/22/2019 09:12:44 - INFO - __main__ -     Batch size = 32\n",
            "04/22/2019 09:12:44 - INFO - __main__ -     Num steps = 0\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A04/22/2019 09:12:44 - INFO - __main__ -   *** Example ***\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   guid: 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   tokens: [CLS] [MASK] 国 国 家 档 案 [MASK] 推 [MASK] 英 国 [MASK] 战 历 史 的 专 题 [MASK] 览 ， 纪 念 北 约 [MASK] [MASK] 70 周 年 ， 柏 林 墙 倒 塌 30 周 年 。 [SEP] [MASK] 案 馆 [MASK] 专 [MASK] 说 ， 他 们 希 望 通 过 这 些 冷 战 档 案 文 件 让 人 们 了 [MASK] 冷 战 年 代 的 秘 密 ， 猜 疑 ， 恐 惧 ， 东 西 方 的 政 治 [MASK] [MASK] [MASK] 意 识 形 [MASK] 对 立 。 [SEP]\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_ids: 101 103 1744 1744 2157 3440 3428 103 2972 103 5739 1744 103 2773 1325 1380 4638 683 7579 103 6229 8024 5279 2573 1266 5276 103 103 8203 1453 2399 8024 3377 3360 1870 948 1847 8114 1453 2399 511 102 103 3428 7667 103 683 103 6432 8024 800 812 2361 3307 6858 6814 6821 763 1107 2773 3440 3428 3152 816 6375 782 812 749 103 1107 2773 2399 807 4638 4908 2166 8024 4339 4542 8024 2607 2672 8024 691 6205 3175 4638 3124 3780 103 103 103 2692 6399 2501 103 2190 4989 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   LM label: [-1, 5739, -1, -1, -1, -1, -1, 7667, -1, 1139, -1, -1, 1107, -1, -1, -1, -1, -1, -1, 2245, -1, -1, -1, -1, -1, -1, 2768, 4989, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3440, -1, -1, 4638, -1, 2157, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 6237, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5165, 2476, 1469, -1, -1, -1, 2578, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "04/22/2019 09:12:44 - INFO - __main__ -   Is next sentence label: 0 \n",
            "04/22/2019 09:12:44 - INFO - __main__ -   *** Example ***\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   guid: 1\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   tokens: [CLS] 档 [MASK] 馆 的 专 家 说 [MASK] 他 们 希 望 通 过 这 些 冷 战 档 案 文 件 让 人 们 [MASK] 解 [MASK] 战 年 代 的 [MASK] 密 ， [MASK] 疑 [MASK] 恐 惧 ##彿 东 西 方 的 政 治 紧 张 和 意 识 形 态 对 立 [MASK] [SEP] [MASK] 览 陈 [MASK] 了 [MASK] [MASK] [MASK] 二 战 时 的 首 相 丘 吉 尔 称 为 [MASK] 淘 气 文 件 [MASK] 的 协 [MASK] 原 件 [MASK] [SEP]\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_ids: 101 3440 103 7667 4638 683 2157 6432 103 800 812 2361 3307 6858 6814 6821 763 1107 2773 3440 3428 3152 816 6375 782 812 103 6237 103 2773 2399 807 4638 103 2166 8024 103 4542 103 2607 2672 15574 691 6205 3175 4638 3124 3780 5165 2476 1469 2692 6399 2501 2578 2190 4989 103 102 103 6229 7357 103 749 103 103 103 753 2773 3198 4638 7674 4685 687 1395 2209 4917 711 103 3905 3698 3152 816 103 4638 1291 103 1333 816 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   LM label: [-1, -1, 3428, -1, -1, -1, -1, -1, 8024, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 749, -1, 1107, -1, -1, -1, -1, 4908, -1, -1, 4339, -1, 8024, -1, -1, 8024, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 511, -1, 2245, -1, -1, 1154, -1, 6158, 5739, 1744, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 100, -1, -1, -1, -1, 100, -1, -1, 6379, -1, -1, 511, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "04/22/2019 09:12:44 - INFO - __main__ -   Is next sentence label: 0 \n",
            "04/22/2019 09:12:44 - INFO - __main__ -   *** Example ***\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   guid: 2\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   tokens: [CLS] 展 览 陈 列 [MASK] 被 英 [MASK] 二 战 [MASK] [MASK] 首 相 丘 吉 尔 称 为 [MASK] 淘 气 文 件 [UNK] 的 协 议 原 件 。 [SEP] [MASK] 今 [MASK] 在 [MASK] 去 世 百 [MASK] 后 ， [MASK] [MASK] 乎 在 英 国 被 [MASK] [MASK] 忘 ， 但 他 的 工 作 qr 就 仍 在 [MASK] 代 中 [MASK] 留 下 ##over 记 。 [SEP]\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_ids: 101 2245 6229 7357 1154 103 6158 5739 103 753 2773 103 103 7674 4685 687 1395 2209 4917 711 103 3905 3698 3152 816 100 4638 1291 6379 1333 816 511 102 103 791 103 1762 103 1343 686 4636 103 1400 8024 103 103 725 1762 5739 1744 6158 103 103 2563 8024 852 800 4638 2339 868 9830 2218 793 1762 103 807 704 103 4522 678 11436 6381 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   LM label: [-1, 2245, -1, -1, -1, 749, -1, -1, 1744, -1, -1, 3198, 4638, -1, -1, -1, -1, -1, -1, -1, 100, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1963, -1, 8024, -1, 800, -1, -1, -1, 2399, -1, -1, 800, 1126, -1, -1, -1, -1, -1, 782, 6890, -1, -1, -1, -1, -1, -1, -1, 2768, -1, -1, -1, 4385, -1, -1, 1744, -1, -1, 1313, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "04/22/2019 09:12:44 - INFO - __main__ -   Is next sentence label: 1 \n",
            "04/22/2019 09:12:44 - INFO - __main__ -   *** Example ***\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   guid: 3\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   tokens: [CLS] 那 是 丘 吉 尔 与 斯 大 林 [MASK] 二 战 即 将 结 束 时 在 东 欧 划 分 势 [MASK] 范 围 [MASK] 成 free [UNK] 百 分 比 [MASK] 议 [UNK] 。 [SEP] [MASK] 是 丘 吉 [MASK] 与 斯 大 [MASK] 在 二 战 即 将 结 束 时 在 东 欧 划 分 势 力 范 围 达 成 的 [UNK] 百 分 比 [MASK] 议 [UNK] 。 [SEP]\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_ids: 101 6929 3221 687 1395 2209 680 3172 1920 3360 103 753 2773 1315 2199 5310 3338 3198 1762 691 3616 1153 1146 1232 103 5745 1741 103 2768 8583 100 4636 1146 3683 103 6379 100 511 102 103 3221 687 1395 103 680 3172 1920 103 1762 753 2773 1315 2199 5310 3338 3198 1762 691 3616 1153 1146 1232 1213 5745 1741 6809 2768 4638 100 4636 1146 3683 103 6379 100 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   LM label: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1762, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1213, -1, -1, 6809, -1, 4638, -1, -1, -1, -1, 1291, -1, -1, 511, -1, 6929, -1, -1, -1, 2209, -1, -1, -1, 3360, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1291, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "04/22/2019 09:12:44 - INFO - __main__ -   Is next sentence label: 1 \n",
            "04/22/2019 09:12:44 - INFO - __main__ -   *** Example ***\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   guid: 4\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   tokens: [CLS] 一 年 多 以 [MASK] ， 丘 吉 尔 在 美 [MASK] 发 表 了 著 [MASK] 的 [UNK] 铁 幕 [UNK] [MASK] 说 ， [MASK] 容 苏 联 和 东 欧 社 会 主 义 国 家 ##する 用 铁 幕 笼 罩 起 来 [UNK] 。 [SEP] 但 一 名 印 度 选 民 说 ， 在 意 识 到 自 己 错 投 选 票 [MASK] 他 不 支 持 的 [MASK] 党 后 ， 他 砍 掉 了 自 己 的 食 指 。 [SEP]\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_ids: 101 671 2399 1914 809 103 8024 687 1395 2209 1762 5401 103 1355 6134 749 5865 103 4638 100 7188 2391 100 103 6432 8024 103 2159 5722 5468 1469 691 3616 4852 833 712 721 1744 2157 9840 4500 7188 2391 5021 5388 6629 3341 100 511 102 852 671 1399 1313 2428 6848 3696 6432 8024 1762 2692 6399 1168 5632 2346 7231 2832 6848 4873 103 800 679 3118 2898 4638 103 1054 1400 8024 800 4775 2957 749 5632 2346 4638 7608 2900 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2019 09:12:44 - INFO - __main__ -   LM label: [-1, -1, -1, -1, -1, 1400, -1, -1, -1, -1, -1, -1, 1744, -1, -1, -1, -1, 1399, -1, -1, -1, -1, -1, 4028, -1, -1, 2501, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 100, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5314, -1, -1, -1, -1, 4638, 3124, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4638, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
            "04/22/2019 09:12:44 - INFO - __main__ -   Is next sentence label: 1 \n",
            "\n",
            "Iteration: 100% 1/1 [00:22<00:00, 22.66s/it]\u001b[A\n",
            "Epoch:  33% 1/3 [00:22<00:45, 22.66s/it]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:21<00:00, 21.68s/it]\u001b[A\n",
            "Epoch:  67% 2/3 [00:44<00:22, 22.37s/it]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:21<00:00, 21.13s/it]\u001b[A\n",
            "Epoch: 100% 3/3 [01:05<00:00, 22.00s/it]\n",
            "04/22/2019 09:13:50 - INFO - __main__ -   ** ** * Saving fine - tuned model ** ** * \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}